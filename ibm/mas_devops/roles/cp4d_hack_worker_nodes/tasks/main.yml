---
# This is a horrible hack, but it's what CP4D tells us to do :(
#
# This should be executed as part of cluster prepararion if you want to use CP4D v4.
#
# It will reboot all worker nodes, causing disruption to the entire cluster and everything
# running on it we do not include this as part of the normal flow because, well it shouldn't
# be necessary to reboot worker nodes to install containerized software.  Hopefully this is just
# an example of poor documentation and there's a simple alternative that we can implement to
# remove this role.
#
# https://cloud.ibm.com/docs/openshift?topic=openshift-registry#cluster_global_pull_secret


# 0. Fail if required params are not provided
# -----------------------------------------------------------------------------
- name: "Fail if cpd_registry_password has not been provided"
  when: cpd_registry_password is not defined or cpd_registry_password == ""
  fail:
    msg: "cpd_registry_password property has not been set"

# 0. Fail if we are trying to do this in an unsupported cluster type
# -----------------------------------------------------------------------------
- name: "Fail if cluster_type is not set to roks"
  when: cluster_type is not defined or cluster_type != "roks"
  fail:
    msg: "cluster_type property has not been set, only "roks" is supported"

- name: "Debug information"
  debug:
    msg:
      - "CPD registry ................. {{ cpd_registry }}"
      - "CPD username ................. {{ cpd_registry_user }}"
      - "Cluster Name ................. {{ cluster_name }}"
      - "Cluster Type ................. {{ cluster_type }}"


# 1. Update cp4d entitlement key and cpd registry server in cluster's pull-secret
# ---------------------------------------------------------------------------------------------------------------------

# 1.1 Generate the new secret
- name: Set new secret content
  vars:
    entitledAuthStr: "{{ cpd_registry_user }}:{{ cpd_registry_password }}"
    entitledAuth: "{{ entitledAuthStr | b64encode }}"
    content:
      - "{\"auths\":{\"{{ cpd_registry }}\":{\"username\":\"{{ cpd_registry_user }}\",\"password\":\"{{ cpd_registry_password }}\",\"email\":\"{{ cpd_registry_user }}\",\"auth\":\"{{ entitledAuth }}\"}"
      - "}"
      - "}"
  set_fact:
    new_secret: "{{ content | join('') }}"

# 1.2 Find the existing secret, and we are going to modify it rather than replace
- name: Retrieve existing pull-secret content
  community.kubernetes.k8s_info:
    api: v1
    kind: Secret
    name: pull-secret
    namespace: openshift-config
  register: pullsecret

- name: Get the original cred secrets
  set_fact:
    original_secret: "{{ item.data }}"
  with_items: "{{ pullsecret.resources }}"
  no_log: true

- name: Get the dockerconfigjson info
  set_fact:
    secret_string: '{{ original_secret[".dockerconfigjson"] | b64decode | from_json }}'

# 1.3 Append our new credentials to the secret
# TODO: This needs to be idempotent .. running this repeatedly shouldn't result in us
#       continually adding new entries, has this been tested?
- name: Combine new secret content
  set_fact:
    new_secret_string: '{{ secret_string | combine( new_secret, recursive=True) }}'

# 1.4. Overwrite the secret
- name: "Create new pull-secret"
  community.kubernetes.k8s:
    definition:
      apiVersion: v1
      kind: Secret
      type: kubernetes.io/dockerconfigjson
      metadata:
        name: pull-secret
        namespace: openshift-config
      data:
        .dockerconfigjson: "{{ new_secret_string | to_json | b64encode }}"


# 2. Reload the cluster worker nodes after changes to pull-secret
# ---------------------------------------------------------------------------------------------------------------------

# 2.1 Obtain a list of worker nodes
- name: "roks : Get worker nodes"
  shell: |
    ibmcloud oc worker ls -c {{ cluster_name }} -q | awk '{print $1}'
  register: cluster_lookup
  failed_when: "cluster_lookup.rc > 1"

# 2.2 Issue reload commands to them all
- name: "roks : Reload nodes"
  vars:
    workers: "{{ item }}"
  when:
    item | length > 0
  shell: |
    ibmcloud oc worker reload -c {{ cluster_name }} -w {{ item }} -f
  with_items: "{{ cluster_lookup.stdout_lines }}"

# 2.3 Wait for all worker nodes to enter "pending" state
- name: "roks: Worker nodes beginning reload process..."
  shell: |
    ibmcloud oc cluster get --cluster {{ cluster_name }} --output json
  register: roks_cluster_completion
  until:
    - roks_cluster_completion.rc == 0
    - (roks_cluster_completion.stdout | from_json).state == 'pending'
  retries: 15
  delay: 60

# 2.4 Wait for all worker nodes to enter back into "normal" state
- name: "roks : Wait until the Roks worker nodes are back"
  shell: |
    ibmcloud oc cluster get --cluster {{ cluster_name }} --output json
  register: roks_cluster_completion
  until:
    - roks_cluster_completion.rc == 0
    - (roks_cluster_completion.stdout | from_json).state == 'normal'
  retries: 60
  delay: 60
