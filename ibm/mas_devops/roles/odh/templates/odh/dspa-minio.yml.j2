---
apiVersion: v1
kind: Secret
metadata:
  name: ds-pipeline-s3-instance
  namespace: mas-{{ mas_instance_id }}-aibroker
  labels:
    opendatahub.io/dashboard: 'false'
    opendatahub.io/managed: 'true'
  annotations:
    opendatahub.io/connection-type: s3
    openshift.io/display-name: Minio Data Connection
stringData:
  # The following keys are needed while https://github.com/kubeflow/pipelines/issues/9689 is open
  accesskey: "{{ storage_accesskey }}"
  secretkey: "{{ storage_secretkey }}"
  # Added for kmodels deployment script
  host: "{{ storage_host }}"
  port: "{{ storage_port }}"
  ssl: "{{ storage_ssl }}"
---
apiVersion: datasciencepipelinesapplications.opendatahub.io/v1alpha1
kind: DataSciencePipelinesApplication
metadata:
  name: instance
  namespace: mas-{{ mas_instance_id }}-aibroker
spec:
  database:
    disableHealthCheck: true
    mariaDB:
      deploy: true
      image: registry.redhat.io/rhel8/mariadb-103:1-188
      username: mlpipeline
      pipelineDBName: randomDBName
      pvcSize: 20Gi
      storageClassName: "{{ primary_storage_class }}"
      resources:
        requests:
          cpu: 300m
          memory: 800Mi
        limits:
          cpu: "1"
          memory: 1Gi
      # requires this configmap to be created before hand,
      # otherwise operator will not deploy DSPA
      passwordSecret:
        name: ds-pipeline-s3-instance
        key: secretkey
  objectStorage:
    externalStorage:
      host: "{{ storage_host }}"
      port: "9000"
      bucket: mlpipeline
      storageClassName: "{{ primary_storage_class }}"
      # region: us-east-2
      s3CredentialsSecret:
        accessKey: accesskey
        secretKey: secretkey
        secretName: ds-pipeline-s3-instance
      scheme: http
